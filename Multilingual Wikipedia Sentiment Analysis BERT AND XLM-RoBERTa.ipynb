{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f93e878-936b-48a1-bad5-c9e693fd5289",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (485859493.py, line 59)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 59\u001b[1;36m\u001b[0m\n\u001b[1;33m    tokenizer=tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\",\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict, Optional, List\n",
    "from functools import partial\n",
    "import logging\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\"\"\"\n",
    "Libraries Used:\n",
    "- wikipediaapi: Retrieve Wikipedia articles\n",
    "- transformers: Sentiment analysis with pre-trained models\n",
    "- pandas: Data management and CSV handling\n",
    "- numpy: Statistical calculations\n",
    "- torch: Machine learning backend\n",
    "- concurrent.futures: Parallel processing\n",
    "\n",
    "Key Features:\n",
    "- Supports multiple languages\n",
    "- Uses GPU or Apple Silicon if available\n",
    "- Parallel text processing\n",
    "- Robust error handling\n",
    "\"\"\"\n",
    "\n",
    "class WikiAnalyzer:\n",
    "    def __init__(self, user_agent: str):\n",
    "        \"\"\"\n",
    "        Setup the Wikipedia sentiment analyzer:\n",
    "        - Initialize Wikipedia API connection\n",
    "        - Select best available computational device (GPU, Apple Silicon, or CPU)\n",
    "        - Configure sentiment analysis model\n",
    "        - Set up logging\n",
    "        \"\"\"\n",
    "        self.wiki = wikipediaapi.Wikipedia(user_agent, 'en')\n",
    "        \n",
    "        # Choose the best available computational device\n",
    "        if torch.cuda.is_available():\n",
    "            device = 0  # Use first GPU\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = \"mps\"  # Use Apple M1/M2 GPU\n",
    "        else:\n",
    "            device = -1  # Fall back to CPU\n",
    "\n",
    "        # BERT:\n",
    "        # model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "        # tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "\n",
    "        # XLM-RoBERTa:\n",
    "        # model=AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "        # tokenizer=AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "            \n",
    "        # Configure sentiment analysis model\n",
    "        self.analyzer = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "            tokenizer=tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # List of supported language codes\n",
    "        self.languages = ['en', 'pl', 'it', 'hu', 'de', 'nl', 'el', 'sv', 'ko', 'th', 'tl', 'ja', 'si', 'bn', 'hi', 'ms', 'he', 'tr']\n",
    "        \n",
    "        # Configure logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Log the computational device in use\n",
    "        device_name = \"GPU\" if device == 0 else \"Apple Silicon\" if device == \"mps\" else \"CPU\"\n",
    "        self.logger.info(f\"Using device: {device_name}\")\n",
    "\n",
    "    def fetch_article_texts(self, article_name: str) -> Dict[str, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Retrieve Wikipedia article content in multiple languages:\n",
    "        - Starts with English article\n",
    "        - Attempts to find translations for other supported languages\n",
    "        - Returns dictionary of language codes and their article texts\n",
    "        \n",
    "        Handles potential errors:\n",
    "        - Logs if an article is not found\n",
    "        - Returns None for unavailable translations\n",
    "        \"\"\"\n",
    "        texts = {}\n",
    "        \n",
    "        # Retrieve English article\n",
    "        page = self.wiki.page(article_name)\n",
    "        if not page.exists():\n",
    "            self.logger.error(f\"en article '{article_name}' not found\")\n",
    "            texts['en'] = None\n",
    "        else:\n",
    "            texts['en'] = page.text\n",
    "            self.logger.info(f\"Found en article: {article_name}\")\n",
    "        \n",
    "        # Retrieve translations for other languages\n",
    "        for lang in self.languages[1:]:  # Skip English\n",
    "            try:\n",
    "                lang_page = page.langlinks.get(lang)\n",
    "                if lang_page:\n",
    "                    texts[lang] = lang_page.text\n",
    "                    self.logger.info(f\"Found {lang} article: {lang_page.title}\")\n",
    "                else:\n",
    "                    self.logger.warning(f\"No {lang} translation available\")\n",
    "                    texts[lang] = None\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error fetching {lang} article: {str(e)}\")\n",
    "                texts[lang] = None\n",
    "                \n",
    "        return texts\n",
    "\n",
    "    def analyze_chunk(self, chunk: str) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Process a text segment for sentiment:\n",
    "        - Runs sentiment analysis on a single chunk of text\n",
    "        - Returns sentiment score\n",
    "        - Handles empty or problematic chunks gracefully\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not chunk.strip():\n",
    "                return None\n",
    "            return self.analyzer(chunk)[0]['score']\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error analyzing chunk: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_sentiment(self, text: str, chunk_size: int = 512) -> Optional[Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Perform comprehensive sentiment analysis:\n",
    "        - Splits text into smaller chunks\n",
    "        - Processes chunks in parallel\n",
    "        - Calculates statistical sentiment measures:\n",
    "          * Average sentiment score\n",
    "          * Median sentiment score\n",
    "          * Standard deviation of scores\n",
    "        \n",
    "        Efficiently handles texts of varying lengths\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return None\n",
    "            \n",
    "        # Split text into manageable chunks\n",
    "        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "        \n",
    "        # Process chunks concurrently\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            scores = list(filter(None, executor.map(self.analyze_chunk, chunks)))\n",
    "            \n",
    "        if not scores:\n",
    "            return None\n",
    "            \n",
    "        return {\n",
    "            'average_score': float(np.mean(scores)),\n",
    "            'median_score': float(np.median(scores)),\n",
    "            'standard_deviation': float(np.std(scores))\n",
    "        }\n",
    "\n",
    "    def create_report(self, article_name: str, results: Dict[str, Optional[Dict[str, float]]], output_file: str = 'sentiment_statistics_bert.csv') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate comprehensive sentiment analysis report:\n",
    "        - Creates structured data from analysis results\n",
    "        - Handles both successful and failed analyses\n",
    "        - Manages CSV file operations:\n",
    "          * Creates new file if needed\n",
    "          * Updates existing file\n",
    "          * Prevents duplicate entries\n",
    "        \n",
    "        Ensures clean and consistent data representation\n",
    "        \"\"\"\n",
    "        report_data = []\n",
    "        \n",
    "        for language, data in results.items():\n",
    "            if data:\n",
    "                report_data.append({\n",
    "                    'Article_Title': article_name,\n",
    "                    'Language': language,\n",
    "                    'Mean_Sentiment_Score': f\"{data['average_score']:.3f}\",\n",
    "                    'Median_Sentiment_Score': f\"{data['median_score']:.3f}\",\n",
    "                    'Standard_Deviation': f\"{data['standard_deviation']:.3f}\"\n",
    "                })\n",
    "            else:\n",
    "                report_data.append({\n",
    "                    'Article_Title': article_name,\n",
    "                    'Language': language,\n",
    "                    'Mean_Sentiment_Score': 'NA',\n",
    "                    'Median_Sentiment_Score': 'NA',\n",
    "                    'Standard_Deviation': 'NA'\n",
    "                })\n",
    "                \n",
    "        stats_df = pd.DataFrame(report_data)\n",
    "        \n",
    "        try:\n",
    "            existing_df = pd.read_csv(output_file)\n",
    "            # Remove any existing entries for this article\n",
    "            existing_df = existing_df[existing_df.Article_Title != article_name]\n",
    "            updated_df = pd.concat([existing_df, stats_df], ignore_index=True)\n",
    "        except FileNotFoundError:\n",
    "            updated_df = stats_df\n",
    "            \n",
    "        updated_df.to_csv(output_file, index=False)\n",
    "        return updated_df\n",
    "\n",
    "    def analyze_article(self, article_name: str, chunk_size: int = 512) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main method to analyze a Wikipedia article:\n",
    "        - Retrieves article texts in multiple languages\n",
    "        - Performs sentiment analysis\n",
    "        - Generates comprehensive report\n",
    "        \n",
    "        Single entry point for full multilingual sentiment analysis\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting analysis of article: {article_name}\")\n",
    "        \n",
    "        # Fetch article texts in different languages\n",
    "        texts = self.fetch_article_texts(article_name)\n",
    "        \n",
    "        # Analyze sentiments for each language\n",
    "        results = {}\n",
    "        for lang, text in texts.items():\n",
    "            self.logger.info(f\"Analyzing sentiment for language: {lang}\")\n",
    "            results[lang] = self.analyze_sentiment(text, chunk_size) if text else None\n",
    "            \n",
    "        # Create and save report\n",
    "        report = self.create_report(article_name, results)\n",
    "        self.logger.info(\"Analysis completed successfully\")\n",
    "        return report\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = WikiAnalyzer('WikiSentimentAnalysis/1.0 (username@email.com)')\n",
    "    \n",
    "    presidents = ['Joe Biden', 'Donald Trump', 'Barack Obama', 'George W. Bush', 'Bill Clinton', 'George H. W. Bush', 'Ronald Reagan', \n",
    "                  'Jimmy Carter', 'Gerald Ford', 'Richard Nixon', 'Lyndon B. Johnson', 'John F. Kennedy', 'Dwight D. Eisenhower', 'Harry S. Truman', \n",
    "                  'Franklin D. Roosevelt', 'Herbert Hoover', 'Calvin Coolidge', 'Warren G. Harding', 'Woodrow Wilson', 'William Howard Taft', \n",
    "                  'Theodore Roosevelt', 'William McKinley', 'Grover Cleveland', 'Benjamin Harrison', 'Chester A. Arthur', 'James A. Garfield', \n",
    "                  'Rutherford B. Hayes', 'Ulysses S. Grant', 'Andrew Johnson', 'Abraham Lincoln', 'James Buchanan', 'Franklin Pierce', \n",
    "                  'Millard Fillmore', 'Zachary Taylor', 'James K. Polk', 'John Tyler', 'William Henry Harrison', 'Martin Van Buren', 'Andrew Jackson', \n",
    "                  'John Quincy Adams', 'James Monroe', 'James Madison', 'Thomas Jefferson', 'John Adams', 'George Washington']\n",
    "    for president in presidents:\n",
    "        report = analyzer.analyze_article(president)\n",
    "    \n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe468c0-b804-470c-8baf-6eca1ae4987c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
