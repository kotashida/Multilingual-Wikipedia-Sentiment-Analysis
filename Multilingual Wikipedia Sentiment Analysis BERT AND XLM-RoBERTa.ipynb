{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae6f364-171b-4fcb-a910-0350c30fa419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 21:15:47,569 - INFO - Wikipedia: language=en, user_agent: WikiSentimentAnalysis/1.0 (username@email.com) (Wikipedia-API/0.7.1; https://github.com/martin-majlis/Wikipedia-API/), extract_format=1\n",
      "2024-12-04 21:15:51,350 - INFO - Using device: CPU\n",
      "2024-12-04 21:15:51,352 - INFO - Starting analysis of article: Joe Biden\n",
      "2024-12-04 21:15:51,353 - INFO - Request URL: https://en.wikipedia.org/w/api.php?action=query&prop=info&titles=Joe Biden&inprop=protection|talkid|watched|watchers|visitingwatchers|notificationtimestamp|subjectid|url|readable|preload|displaytitle\n",
      "2024-12-04 21:15:51,715 - INFO - Request URL: https://en.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Joe Biden&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:51,933 - INFO - Found en article: Joe Biden\n",
      "2024-12-04 21:15:51,934 - INFO - Request URL: https://en.wikipedia.org/w/api.php?action=query&prop=langlinks&titles=Joe Biden&lllimit=500&llprop=url\n",
      "2024-12-04 21:15:52,099 - INFO - Request URL: https://pl.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Joe Biden&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:52,286 - INFO - Found pl article: Joe Biden\n",
      "2024-12-04 21:15:52,287 - INFO - Request URL: https://it.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Joe Biden&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:52,540 - INFO - Found it article: Joe Biden\n",
      "2024-12-04 21:15:52,541 - INFO - Request URL: https://hu.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Joe Biden&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:52,778 - INFO - Found hu article: Joe Biden\n",
      "2024-12-04 21:15:52,780 - INFO - Request URL: https://de.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Joe Biden&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:53,035 - INFO - Found de article: Joe Biden\n",
      "2024-12-04 21:15:53,035 - INFO - Request URL: https://nl.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Joe Biden&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:53,251 - INFO - Found nl article: Joe Biden\n",
      "2024-12-04 21:15:53,252 - INFO - Request URL: https://el.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Τζο Μπάιντεν&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:53,580 - INFO - Found el article: Τζο Μπάιντεν\n",
      "2024-12-04 21:15:53,581 - INFO - Request URL: https://sv.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Joe Biden&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:53,781 - INFO - Found sv article: Joe Biden\n",
      "2024-12-04 21:15:53,782 - INFO - Request URL: https://ko.wikipedia.org/w/api.php?action=query&prop=extracts&titles=조 바이든&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:53,975 - INFO - Found ko article: 조 바이든\n",
      "2024-12-04 21:15:53,976 - INFO - Request URL: https://th.wikipedia.org/w/api.php?action=query&prop=extracts&titles=โจ ไบเดิน&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:54,237 - INFO - Found th article: โจ ไบเดิน\n",
      "2024-12-04 21:15:54,238 - INFO - Request URL: https://tl.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Joe Biden&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:54,482 - INFO - Found tl article: Joe Biden\n",
      "2024-12-04 21:15:54,483 - INFO - Request URL: https://ja.wikipedia.org/w/api.php?action=query&prop=extracts&titles=ジョー・バイデン&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:54,874 - INFO - Found ja article: ジョー・バイデン\n",
      "2024-12-04 21:15:54,875 - INFO - Request URL: https://si.wikipedia.org/w/api.php?action=query&prop=extracts&titles=ජෝ බයිඩන්&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:55,013 - INFO - Found si article: ජෝ බයිඩන්\n",
      "2024-12-04 21:15:55,014 - INFO - Request URL: https://bn.wikipedia.org/w/api.php?action=query&prop=extracts&titles=জো বাইডেন&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:55,296 - INFO - Found bn article: জো বাইডেন\n",
      "2024-12-04 21:15:55,297 - INFO - Request URL: https://hi.wikipedia.org/w/api.php?action=query&prop=extracts&titles=जो बाइडन&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:55,490 - INFO - Found hi article: जो बाइडन\n",
      "2024-12-04 21:15:55,490 - INFO - Request URL: https://ms.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Joe Biden&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:55,729 - INFO - Found ms article: Joe Biden\n",
      "2024-12-04 21:15:55,731 - INFO - Request URL: https://he.wikipedia.org/w/api.php?action=query&prop=extracts&titles=ג'ו ביידן&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:56,180 - INFO - Found he article: ג'ו ביידן\n",
      "2024-12-04 21:15:56,181 - INFO - Request URL: https://tr.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Joe Biden&explaintext=1&exsectionformat=wiki\n",
      "2024-12-04 21:15:56,417 - INFO - Found tr article: Joe Biden\n",
      "2024-12-04 21:15:56,419 - INFO - Analyzing sentiment for language: en\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict, Optional, List\n",
    "from functools import partial\n",
    "import logging\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\"\"\"\n",
    "Key dependencies:\n",
    "1. wikipediaapi: Wikipedia content access\n",
    "2. transformers: BERT sentiment analysis\n",
    "3. pandas: Data manipulation and CSV handling\n",
    "4. numpy: Statistical calculations\n",
    "5. torch: Deep learning backend\n",
    "6. concurrent.futures: Parallel processing\n",
    "\n",
    "Performance optimizations:\n",
    "1. GPU acceleration when available\n",
    "2. Apple Silicon support\n",
    "3. Parallel text processing\n",
    "4. Efficient memory handling for large texts\n",
    "\n",
    "Error handling:\n",
    "1. Graceful degradation for missing articles\n",
    "2. Robust logging system\n",
    "3. Exception handling at multiple levels\n",
    "4. None returns instead of crashes\n",
    "\"\"\"\n",
    "\n",
    "class WikiAnalyzer:\n",
    "    def __init__(self, user_agent: str):\n",
    "        \"\"\"\n",
    "        Initializes core components:\n",
    "        1. Wikipedia API client with user agent\n",
    "        2. sentiment analysis pipeline\n",
    "        3. Device selection logic (GPU/Apple Silicon/CPU)\n",
    "        4. Supported language list\n",
    "        5. Logging configuration\n",
    "        \"\"\"\n",
    "        self.wiki = wikipediaapi.Wikipedia(user_agent, 'en')\n",
    "        \n",
    "        # Determine the best available device\n",
    "        if torch.cuda.is_available():\n",
    "            device = 0  # Use first GPU if available\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = \"mps\"  # Use Apple M1/M2 GPU if available\n",
    "        else:\n",
    "            device = -1  # Use CPU\n",
    "            \n",
    "        # BERT:\n",
    "        # model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "        # tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "\n",
    "        # XLM-RoBERTa:\n",
    "        # model=AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "        # tokenizer=AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "\n",
    "        self.analyzer = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"),\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        self.languages = ['en', 'pl', 'it', 'hu', 'de', 'nl', 'el', 'sv', 'ko', 'th', 'tl', 'ja', 'si', 'bn', 'hi', 'ms', 'he', 'tr']\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Log which device is being used\n",
    "        device_name = \"GPU\" if device == 0 else \"Apple Silicon\" if device == \"mps\" else \"CPU\"\n",
    "        self.logger.info(f\"Using device: {device_name}\")\n",
    "\n",
    "    def fetch_article_texts(self, article_name: str) -> Dict[str, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Retrieves article content in all supported languages:\n",
    "        1. First fetches English version\n",
    "        2. Then attempts to get translations for all other supported languages\n",
    "        3. Returns dictionary mapping language codes to article texts\n",
    "        \n",
    "        Error handling:\n",
    "        - Logs failed retrievals\n",
    "        - Returns None for unavailable translations\n",
    "        - Continues processing even if some languages fail\n",
    "        \"\"\"\n",
    "        texts = {}\n",
    "        \n",
    "        # Get English article first\n",
    "        page = self.wiki.page(article_name)\n",
    "        if not page.exists():\n",
    "            self.logger.error(f\"en article '{article_name}' not found\")\n",
    "            texts['en'] = None\n",
    "        else:\n",
    "            texts['en'] = page.text\n",
    "            self.logger.info(f\"Found en article: {article_name}\")\n",
    "        \n",
    "        # Get translations\n",
    "        for lang in self.languages[1:]:  # Skip 'en' as we already have it\n",
    "            try:\n",
    "                lang_page = page.langlinks.get(lang)\n",
    "                if lang_page:\n",
    "                    texts[lang] = lang_page.text\n",
    "                    self.logger.info(f\"Found {lang} article: {lang_page.title}\")\n",
    "                else:\n",
    "                    self.logger.warning(f\"No {lang} translation available\")\n",
    "                    texts[lang] = None\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error fetching {lang} article: {str(e)}\")\n",
    "                texts[lang] = None\n",
    "                \n",
    "        return texts\n",
    "\n",
    "    def analyze_chunk(self, chunk: str) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Processes individual text segments:\n",
    "        1. Takes a single chunk of text (default 512 chars)\n",
    "        2. Runs BERT sentiment analysis\n",
    "        3. Returns sentiment score or None if analysis fails\n",
    "        \n",
    "        Performance consideration:\n",
    "        - Used as a worker function for parallel processing\n",
    "        - Handles empty chunks gracefully\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not chunk.strip():\n",
    "                return None\n",
    "            return self.analyzer(chunk)[0]['score']\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error analyzing chunk: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_sentiment(self, text: str, chunk_size: int = 512) -> Optional[Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Performs parallel sentiment analysis:\n",
    "        1. Splits text into manageable chunks\n",
    "        2. Processes chunks concurrently using ThreadPoolExecutor\n",
    "        3. Calculates statistical measures:\n",
    "           - Average sentiment score\n",
    "           - Median sentiment score\n",
    "           - Standard deviation of scores\n",
    "        \n",
    "        Optimization features:\n",
    "        - Parallel processing for better performance\n",
    "        - Configurable chunk size\n",
    "        - Filters out None results from failed analyses\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return None\n",
    "            \n",
    "        # Split text into chunks\n",
    "        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "        \n",
    "        # Process chunks in parallel\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            scores = list(filter(None, executor.map(self.analyze_chunk, chunks)))\n",
    "            \n",
    "        if not scores:\n",
    "            return None\n",
    "            \n",
    "        return {\n",
    "            'average_score': float(np.mean(scores)),\n",
    "            'median_score': float(np.median(scores)),\n",
    "            'standard_deviation': float(np.std(scores))\n",
    "        }\n",
    "\n",
    "    def create_report(self, article_name: str, results: Dict[str, Optional[Dict[str, float]]], output_file: str = 'sentiment_statistics_bert.csv') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Change csv file name as needed\n",
    "        \n",
    "        Generates analysis reports:\n",
    "        1. Creates structured data from analysis results\n",
    "        2. Handles both successful and failed analyses\n",
    "        3. Manages CSV file operations:\n",
    "           - Creates new file if needed\n",
    "           - Updates existing file while preserving other entries\n",
    "           - Removes duplicate entries for same article\n",
    "        \n",
    "        Data handling:\n",
    "        - Formats floating point numbers to 3 decimal places\n",
    "        - Uses 'NA' for missing data\n",
    "        - Maintains consistent CSV structure\n",
    "        \"\"\"\n",
    "        report_data = []\n",
    "        \n",
    "        for language, data in results.items():\n",
    "            if data:\n",
    "                report_data.append({\n",
    "                    'Article_Title': article_name,\n",
    "                    'Language': language,\n",
    "                    'Mean_Sentiment_Score': f\"{data['average_score']:.3f}\",\n",
    "                    'Median_Sentiment_Score': f\"{data['median_score']:.3f}\",\n",
    "                    'Standard_Deviation': f\"{data['standard_deviation']:.3f}\"\n",
    "                })\n",
    "            else:\n",
    "                report_data.append({\n",
    "                    'Article_Title': article_name,\n",
    "                    'Language': language,\n",
    "                    'Mean_Sentiment_Score': 'NA',\n",
    "                    'Median_Sentiment_Score': 'NA',\n",
    "                    'Standard_Deviation': 'NA'\n",
    "                })\n",
    "                \n",
    "        stats_df = pd.DataFrame(report_data)\n",
    "        \n",
    "        try:\n",
    "            existing_df = pd.read_csv(output_file)\n",
    "            # Remove any existing entries for this article\n",
    "            existing_df = existing_df[existing_df.Article_Title != article_name]\n",
    "            updated_df = pd.concat([existing_df, stats_df], ignore_index=True)\n",
    "        except FileNotFoundError:\n",
    "            updated_df = stats_df\n",
    "            \n",
    "        updated_df.to_csv(output_file, index=False)\n",
    "        return updated_df\n",
    "\n",
    "    def analyze_article(self, article_name: str, chunk_size: int = 512) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main orchestration method:\n",
    "        1. Coordinates entire analysis process:\n",
    "           - Fetches articles in all languages\n",
    "           - Runs sentiment analysis\n",
    "           - Generates final report\n",
    "        2. Provides logging throughout process\n",
    "        3. Returns complete analysis as DataFrame\n",
    "        \n",
    "        Integration features:\n",
    "        - Single entry point for full analysis\n",
    "        - Configurable chunk size\n",
    "        - Comprehensive logging of process\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting analysis of article: {article_name}\")\n",
    "        \n",
    "        # Fetch texts\n",
    "        texts = self.fetch_article_texts(article_name)\n",
    "        \n",
    "        # Analyze sentiments\n",
    "        results = {}\n",
    "        for lang, text in texts.items():\n",
    "            self.logger.info(f\"Analyzing sentiment for language: {lang}\")\n",
    "            results[lang] = self.analyze_sentiment(text, chunk_size) if text else None\n",
    "            \n",
    "        # Create report\n",
    "        report = self.create_report(article_name, results)\n",
    "        self.logger.info(\"Analysis completed successfully\")\n",
    "        return report\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = WikiAnalyzer('WikiSentimentAnalysis/1.0 (username@email.com)')\n",
    "    \n",
    "    presidents = ['Joe Biden', 'Donald Trump', 'Barack Obama', 'George W. Bush', 'Bill Clinton', 'George H. W. Bush', 'Ronald Reagan', \n",
    "                  'Jimmy Carter', 'Gerald Ford', 'Richard Nixon', 'Lyndon B. Johnson', 'John F. Kennedy', 'Dwight D. Eisenhower', 'Harry S. Truman', \n",
    "                  'Franklin D. Roosevelt', 'Herbert Hoover', 'Calvin Coolidge', 'Warren G. Harding', 'Woodrow Wilson', 'William Howard Taft', \n",
    "                  'Theodore Roosevelt', 'William McKinley', 'Grover Cleveland', 'Benjamin Harrison', 'Chester A. Arthur', 'James A. Garfield', \n",
    "                  'Rutherford B. Hayes', 'Ulysses S. Grant', 'Andrew Johnson', 'Abraham Lincoln', 'James Buchanan', 'Franklin Pierce', \n",
    "                  'Millard Fillmore', 'Zachary Taylor', 'James K. Polk', 'John Tyler', 'William Henry Harrison', 'Martin Van Buren', 'Andrew Jackson', \n",
    "                  'John Quincy Adams', 'James Monroe', 'James Madison', 'Thomas Jefferson', 'John Adams', 'George Washington']\n",
    "    for president in presidents:\n",
    "        report = analyzer.analyze_article(president)\n",
    "    \n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93e878-936b-48a1-bad5-c9e693fd5289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict, Optional, List\n",
    "from functools import partial\n",
    "import logging\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\"\"\"\n",
    "Libraries Used:\n",
    "- wikipediaapi: Retrieve Wikipedia articles\n",
    "- transformers: Sentiment analysis with pre-trained models\n",
    "- pandas: Data management and CSV handling\n",
    "- numpy: Statistical calculations\n",
    "- torch: Machine learning backend\n",
    "- concurrent.futures: Parallel processing\n",
    "\n",
    "Key Features:\n",
    "- Supports multiple languages\n",
    "- Uses GPU or Apple Silicon if available\n",
    "- Parallel text processing\n",
    "- Robust error handling\n",
    "\"\"\"\n",
    "\n",
    "class WikiAnalyzer:\n",
    "    def __init__(self, user_agent: str):\n",
    "        \"\"\"\n",
    "        Setup the Wikipedia sentiment analyzer:\n",
    "        - Initialize Wikipedia API connection\n",
    "        - Select best available computational device (GPU, Apple Silicon, or CPU)\n",
    "        - Configure sentiment analysis model\n",
    "        - Set up logging\n",
    "        \"\"\"\n",
    "        self.wiki = wikipediaapi.Wikipedia(user_agent, 'en')\n",
    "        \n",
    "        # Choose the best available computational device\n",
    "        if torch.cuda.is_available():\n",
    "            device = 0  # Use first GPU\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = \"mps\"  # Use Apple M1/M2 GPU\n",
    "        else:\n",
    "            device = -1  # Fall back to CPU\n",
    "            \n",
    "        # Configure sentiment analysis model\n",
    "        self.analyzer = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"),\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # List of supported language codes\n",
    "        self.languages = ['en', 'pl', 'it', 'hu', 'de', 'nl', 'el', 'sv', 'ko', 'th', 'tl', 'ja', 'si', 'bn', 'hi', 'ms', 'he', 'tr']\n",
    "        \n",
    "        # Configure logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Log the computational device in use\n",
    "        device_name = \"GPU\" if device == 0 else \"Apple Silicon\" if device == \"mps\" else \"CPU\"\n",
    "        self.logger.info(f\"Using device: {device_name}\")\n",
    "\n",
    "    def fetch_article_texts(self, article_name: str) -> Dict[str, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Retrieve Wikipedia article content in multiple languages:\n",
    "        - Starts with English article\n",
    "        - Attempts to find translations for other supported languages\n",
    "        - Returns dictionary of language codes and their article texts\n",
    "        \n",
    "        Handles potential errors:\n",
    "        - Logs if an article is not found\n",
    "        - Returns None for unavailable translations\n",
    "        \"\"\"\n",
    "        texts = {}\n",
    "        \n",
    "        # Retrieve English article\n",
    "        page = self.wiki.page(article_name)\n",
    "        if not page.exists():\n",
    "            self.logger.error(f\"en article '{article_name}' not found\")\n",
    "            texts['en'] = None\n",
    "        else:\n",
    "            texts['en'] = page.text\n",
    "            self.logger.info(f\"Found en article: {article_name}\")\n",
    "        \n",
    "        # Retrieve translations for other languages\n",
    "        for lang in self.languages[1:]:  # Skip English\n",
    "            try:\n",
    "                lang_page = page.langlinks.get(lang)\n",
    "                if lang_page:\n",
    "                    texts[lang] = lang_page.text\n",
    "                    self.logger.info(f\"Found {lang} article: {lang_page.title}\")\n",
    "                else:\n",
    "                    self.logger.warning(f\"No {lang} translation available\")\n",
    "                    texts[lang] = None\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error fetching {lang} article: {str(e)}\")\n",
    "                texts[lang] = None\n",
    "                \n",
    "        return texts\n",
    "\n",
    "    def analyze_chunk(self, chunk: str) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Process a text segment for sentiment:\n",
    "        - Runs sentiment analysis on a single chunk of text\n",
    "        - Returns sentiment score\n",
    "        - Handles empty or problematic chunks gracefully\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not chunk.strip():\n",
    "                return None\n",
    "            return self.analyzer(chunk)[0]['score']\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error analyzing chunk: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_sentiment(self, text: str, chunk_size: int = 512) -> Optional[Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Perform comprehensive sentiment analysis:\n",
    "        - Splits text into smaller chunks\n",
    "        - Processes chunks in parallel\n",
    "        - Calculates statistical sentiment measures:\n",
    "          * Average sentiment score\n",
    "          * Median sentiment score\n",
    "          * Standard deviation of scores\n",
    "        \n",
    "        Efficiently handles texts of varying lengths\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return None\n",
    "            \n",
    "        # Split text into manageable chunks\n",
    "        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "        \n",
    "        # Process chunks concurrently\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            scores = list(filter(None, executor.map(self.analyze_chunk, chunks)))\n",
    "            \n",
    "        if not scores:\n",
    "            return None\n",
    "            \n",
    "        return {\n",
    "            'average_score': float(np.mean(scores)),\n",
    "            'median_score': float(np.median(scores)),\n",
    "            'standard_deviation': float(np.std(scores))\n",
    "        }\n",
    "\n",
    "    def create_report(self, article_name: str, results: Dict[str, Optional[Dict[str, float]]], output_file: str = 'sentiment_statistics_bert.csv') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate comprehensive sentiment analysis report:\n",
    "        - Creates structured data from analysis results\n",
    "        - Handles both successful and failed analyses\n",
    "        - Manages CSV file operations:\n",
    "          * Creates new file if needed\n",
    "          * Updates existing file\n",
    "          * Prevents duplicate entries\n",
    "        \n",
    "        Ensures clean and consistent data representation\n",
    "        \"\"\"\n",
    "        report_data = []\n",
    "        \n",
    "        for language, data in results.items():\n",
    "            if data:\n",
    "                report_data.append({\n",
    "                    'Article_Title': article_name,\n",
    "                    'Language': language,\n",
    "                    'Mean_Sentiment_Score': f\"{data['average_score']:.3f}\",\n",
    "                    'Median_Sentiment_Score': f\"{data['median_score']:.3f}\",\n",
    "                    'Standard_Deviation': f\"{data['standard_deviation']:.3f}\"\n",
    "                })\n",
    "            else:\n",
    "                report_data.append({\n",
    "                    'Article_Title': article_name,\n",
    "                    'Language': language,\n",
    "                    'Mean_Sentiment_Score': 'NA',\n",
    "                    'Median_Sentiment_Score': 'NA',\n",
    "                    'Standard_Deviation': 'NA'\n",
    "                })\n",
    "                \n",
    "        stats_df = pd.DataFrame(report_data)\n",
    "        \n",
    "        try:\n",
    "            existing_df = pd.read_csv(output_file)\n",
    "            # Remove any existing entries for this article\n",
    "            existing_df = existing_df[existing_df.Article_Title != article_name]\n",
    "            updated_df = pd.concat([existing_df, stats_df], ignore_index=True)\n",
    "        except FileNotFoundError:\n",
    "            updated_df = stats_df\n",
    "            \n",
    "        updated_df.to_csv(output_file, index=False)\n",
    "        return updated_df\n",
    "\n",
    "    def analyze_article(self, article_name: str, chunk_size: int = 512) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main method to analyze a Wikipedia article:\n",
    "        - Retrieves article texts in multiple languages\n",
    "        - Performs sentiment analysis\n",
    "        - Generates comprehensive report\n",
    "        \n",
    "        Single entry point for full multilingual sentiment analysis\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting analysis of article: {article_name}\")\n",
    "        \n",
    "        # Fetch article texts in different languages\n",
    "        texts = self.fetch_article_texts(article_name)\n",
    "        \n",
    "        # Analyze sentiments for each language\n",
    "        results = {}\n",
    "        for lang, text in texts.items():\n",
    "            self.logger.info(f\"Analyzing sentiment for language: {lang}\")\n",
    "            results[lang] = self.analyze_sentiment(text, chunk_size) if text else None\n",
    "            \n",
    "        # Create and save report\n",
    "        report = self.create_report(article_name, results)\n",
    "        self.logger.info(\"Analysis completed successfully\")\n",
    "        return report\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = WikiAnalyzer('WikiSentimentAnalysis/1.0 (username@email.com)')\n",
    "    \n",
    "    presidents = ['Joe Biden', 'Donald Trump', 'Barack Obama', 'George W. Bush', 'Bill Clinton', 'George H. W. Bush', 'Ronald Reagan', \n",
    "                  'Jimmy Carter', 'Gerald Ford', 'Richard Nixon', 'Lyndon B. Johnson', 'John F. Kennedy', 'Dwight D. Eisenhower', 'Harry S. Truman', \n",
    "                  'Franklin D. Roosevelt', 'Herbert Hoover', 'Calvin Coolidge', 'Warren G. Harding', 'Woodrow Wilson', 'William Howard Taft', \n",
    "                  'Theodore Roosevelt', 'William McKinley', 'Grover Cleveland', 'Benjamin Harrison', 'Chester A. Arthur', 'James A. Garfield', \n",
    "                  'Rutherford B. Hayes', 'Ulysses S. Grant', 'Andrew Johnson', 'Abraham Lincoln', 'James Buchanan', 'Franklin Pierce', \n",
    "                  'Millard Fillmore', 'Zachary Taylor', 'James K. Polk', 'John Tyler', 'William Henry Harrison', 'Martin Van Buren', 'Andrew Jackson', \n",
    "                  'John Quincy Adams', 'James Monroe', 'James Madison', 'Thomas Jefferson', 'John Adams', 'George Washington']\n",
    "    for president in presidents:\n",
    "        report = analyzer.analyze_article(president)\n",
    "    \n",
    "    print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
